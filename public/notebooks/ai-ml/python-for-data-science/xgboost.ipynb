{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost & Ensemble Learning\n",
    "## Ensemble Learning\n",
    "Use many models to solve a single problem.  \n",
    "\n",
    "### Bagging\n",
    "**Bagging**, bootstrap aggregating, uses multiple random subsets/samples of the source data to train on. \n",
    "\n",
    "### Boosting\n",
    "**Boosting** is running several sequential models in an ensemble of models in such a way that each model fixes errors made in the previous model. AdaBoost, XGBoost, and Gradient Boosting Machines are all examples of boosting.  \n",
    "\n",
    "### Stacking\n",
    "**Stacking** combines many models and trains a \"meta\" model.  \n",
    "\n",
    "### Voting\n",
    "Voting classifiers combine predictions from several models by majority voting or by averaging probabilities together.\n",
    "\n",
    "### Advanced Ensemble Learning\n",
    "Bayes Optimal Classifier.  \n",
    "Bayesian Parameter Averaging.  \n",
    "Bayesian Model Combination.  \n",
    "These are, apparently, complex approaches to \"figuring out\" which ensemble learning approach is best.  \n",
    "\n",
    "\n",
    "## XGBoost\n",
    "Extreme gradient boosted trees.  \n",
    "xgboost is a python library: `pip install xgboost`.  \n",
    "Apparently xgboost has helped win kaggle competition.  \n",
    "\n",
    "## UsingXGBoost to leverage Ensemble Learning\n",
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Here, we're using the `iris` dataset (_a small dataset about flowers!_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "4\n",
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "numSamples, numFeatures = iris.data.shape\n",
    "print(numSamples)\n",
    "print(numFeatures)\n",
    "print(list(iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format The Data for XGBoost\n",
    "XGBoost ??prefers/requires?? the input data to be in the DMatrix format.  \n",
    "Luckily the XGBoost library has a `DMatrix` method that can do just that!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup XGBoost Hyper-Parameters\n",
    "setting up [hyper parameters](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster) and [learning task parameters](https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters):\n",
    "- `max_depth`: represents the maximum \"depth\" of the tree. defaults to `6`\n",
    "- `eta`: the step size used during shrinking & updating the model, used to prevent over-fitting. defaults to `.3`\n",
    "- `objective`: here the value instructs xgboost to do multi-class classification with the `softmax`. This requires setting `num_class` as well\n",
    "- `num_class`: the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 4,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3} \n",
    "epochs = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, train, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
      " 2. 0. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Accuracy score\n",
    "using the sklearn lib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0.  \n",
    "That. is. perfect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
