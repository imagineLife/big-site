{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "An AI \"agent\" (_process, etc_) learns to make decisions based on the agent's learning of it's \"environment\".  \n",
    "The process \"explores\" the possible environmental \"states\" and \"actions\".  \n",
    "\n",
    "`Q-Learning` is an implementation of Reinforcement learning:\n",
    "- environmental states as variable `s`\n",
    "- actions in each state as `a`\n",
    "- A value for each action/state combination: `Q`\n",
    "- The \"q value\" starts at `0`, and as the process \"learns\" the `q` values for each action/state, the q-value changes: negatively when \"negative\" outcomes, and positive when positive outcomes\n",
    "\n",
    "## The Inefficient Exploration Problem\n",
    "Exploring can be inefficient. The more options available in the environment, the longer it will take to \"master\" the environment & decided on the \"best\" choices to make at each state.  \n",
    "\n",
    "### Markov Decision Processes\n",
    "Perhaps a simple concept: a framework for decision-making where outcomes are parly random, and partly under the control of a decision-maker.\n",
    "\n",
    "## gym\n",
    "`gym`, in python, is a lib that can allow for building a reinforcement-learning process.  \n",
    "Note: for this particular file to run use `pip3 install --force-reinstall -v gym==0.15.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Taxi Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "# render_mode=\"ansi\"\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "streets.reset()\n",
    "print(streets.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breadkown the UI\n",
    "Let's break down what we're seeing here:\n",
    "\n",
    "-  **R, G, B, and Y** are pickup or dropoff locations for a person\n",
    "-  The **BLUE** letter indicates where we need to pick someone up **from**\n",
    "-  The **MAGENTA** letter indicates where that passenger wants to go **to**\n",
    "-  The **solid lines** represent walls that the taxi cannot cross.\n",
    "-  The **filled rectangle** represents the taxi itself - it's yellow when empty, and green when carrying a passenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding State\n",
    "Our little world here, which we've called \"streets\", is a 5x5 grid.  \n",
    "The state of this world at any time can be defined by:\n",
    "\n",
    "-  **Where the taxi is** (one of 5x5 = 25 locations)\n",
    "-  **What the current destination is** (4 possibilities)\n",
    "-  **Where the passenger is** (5 possibilities: at one of the destinations, or inside the taxi)\n",
    "\n",
    "So there are a total of 25 x 4 x 5 = 500 possible states that describe our world.\n",
    "\n",
    "### Understanding Actions\n",
    "For each state, there are six possible actions:\n",
    "\n",
    "-  **Move** South, East, North, or West\n",
    "-  **Pickup** a passenger\n",
    "-  **Drop off** a passenger\n",
    "\n",
    "### Understanding Rewards + Penalties\n",
    "Q-Learning will take place using the following rewards and penalties at each state:\n",
    "\n",
    "-  A successfull drop-off yields **+20 points**\n",
    "-  Every time step taken while driving a passenger yields a **-1 point penalty**\n",
    "-  Picking up or dropping off at an illegal location yields a **-10 point penalty**\n",
    "\n",
    "Moving across a wall just isn't allowed at all.\n",
    "\n",
    "## Defining Initial State\n",
    "Let's define an initial state:\n",
    "- taxi at location (x=2, y=3),\n",
    "- the passenger at pickup location 2 \n",
    "- the destination at location 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# creating initial taxi location\n",
    "startingX = 2\n",
    "startingY = 3\n",
    "passengerLocation = 2\n",
    "pickupLocation = 0\n",
    "initial_state = streets.encode(startingX, startingY, passengerLocation, pickupLocation)\n",
    "\n",
    "streets.s = initial_state\n",
    "\n",
    "print(streets.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gym provides location awards preview\n",
    "[some docs](https://gymnasium.farama.org/environments/toy_text/taxi/)  \n",
    "The gym library, for this \"taxi\" environment, provides a \"reward table\" by passing a state to the `P` method. The `P` method returns a dictionary, where each available action, here 6 actions (_move south/north/east/west, pickup, dropoff_), gets a key in the dictionary. each value of each dictionary key contains the probabilities that correlate with each action:\n",
    "- the \"learned\" probability value of taking said action on said state\n",
    "- the next \"state\"\n",
    "- the \"reward\" value\n",
    "- if the action ends in a successful drop-off for that state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streets.P[initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards In-Depth\n",
    "Here's how to interpret this: \n",
    "- each row corresponds to a potential action at this state:\n",
    "  - move South, North, East, or West, pickup, or dropoff\n",
    "- The four values in each row are... (_looking at the first row as an example_)\n",
    "  - the probability assigned to that action (_1.0_)\n",
    "  - the next state that results from that action (_368_)\n",
    "  - the reward for that action (_-1_)\n",
    "  - and whether that action indicates a successful dropoff took place (_False_)\n",
    "\n",
    "Moving North from this state would put us into state number 368, incur a penalty of -1 for taking up time, and does not result in a successful dropoff.\n",
    "\n",
    "\n",
    "## Train The Model\n",
    "So, let's do Q-learning! First we need to train our model. At a high level, we'll train over 10,000 simulated taxi runs. For each run, we'll step through time, with a 10% chance at each step of making a random, exploratory step instead of using the learned Q values to guide our actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.6\n",
    "exploration = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample() # Explore a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "            \n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        \n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        q_table[state, action] = new_q\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the trained model's initial state\n",
    "So now we have a table of Q-values that can be quickly used to determine the optimal next step for any given state! Let's check the table for our initial state above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4033722 , -2.39842765, -2.40161804, -2.3639511 , -9.20582162,\n",
       "       -7.31525097])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest q-value here corresponds to the action \"go West\", which makes sense - that's the most direct route toward our destination from that point. It seems to work! \n",
    "\n",
    "## Run The Simulation Using The Trained Model\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10 Step 17\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()   \n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, *_ = streets.step(action)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "        print(streets.render())\n",
    "        sleep(.1)\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "        \n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
